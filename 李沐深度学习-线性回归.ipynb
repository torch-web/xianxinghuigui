{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)  #注意是双下划线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.ones((2,5,4))\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5., 5., 5., 5.],\n",
       "        [5., 5., 5., 5.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sum(axis=1)  #axis=1，消除Size([2, 5, 4])中索引为1的那个维度，将5消除，变成2*4的矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自动求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= torch.arange(4.0)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算y关于x的梯度之前，需要一个地方储存梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.], requires_grad=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.requires_grad_(True) #等价于 x= torch.arange（4.0，requires_grad=True）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 2*torch.dot(x,x) #x的内积乘2\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过反向传播计算y关于x每个分量的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == 4*x # y关于x的倒数为4*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在默认情况下，pytorch会积累梯度, x.grad.zero_()清除梯度\n",
    "x.grad.zero_()\n",
    "y = x.sum()\n",
    "y.backward()\n",
    "x.grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果没有x.grad.zero_()梯度清0,会将两次的梯度相加tensor([ 0.,  4.,  8., 12.]) + tensor([1., 1., 1., 1.])\n",
    "= tensor([1., 5., 9., 13.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.线性回归\n",
    " 1.1 生成数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import random  #随机数\n",
    "import torch \n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_data(w, b, num_examples):  \n",
    "    \"\"\"生成 y=Xw + b + 噪声\"\"\"\n",
    "    X = torch.normal(0, 1, (num_examples, len(w))) # 正态分布（均值为0，标准差为1），num_examples：n个样本，w：列数\n",
    "    y = torch.matmul(X, w) + b # 矩阵相乘\n",
    "    y += torch.normal(0, 0.01, y.shape) # 加入噪声项\n",
    "    # 得到的y为行向量的形式，为了使其变为一列的形式需要进行reshape\n",
    "    return X, y.reshape((-1, 1))  # x,y从行向量变为列向量\n",
    "\n",
    "true_w=torch.tensor([2,-3.4])\n",
    "true_b=4.2\n",
    "features, labels = synthetic_data(true_w,true_b,1000)\n",
    "# features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 读取数据\n",
    "定义一个data_iter函数，该函数接受批量大小，特征矩阵和标签向量作为输入，生成大小为batch_size的小批量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples)) # indices是一个序列\n",
    "    # 这些样本是随机读出的，没有特定的顺序\n",
    "    random.shuffle(indices)  #将下标随机打乱\n",
    "    for i in range(0, num_examples, batch_size): # 从0开始到最后，步长batch_size\n",
    "        batch_indices = torch.tensor(indices[i:min(i+batch_size,num_examples)])\n",
    "#         print(batch_indices)\n",
    "#         print(\"----------\")\n",
    "        # min(i+batch_size,num_examples) 在i+batch_size和num_examples中取一个最小数，\n",
    "        # 防止最后一个batch数据量不够，从而报错\n",
    "        yield features[batch_indices],labels[batch_indices]\n",
    "        \n",
    "batch_size = 10\n",
    "# for X,y in data_iter(batch_size, features, labels):\n",
    "#     print(X,'\\n',y)\n",
    "#     print(\"+++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 初始化模型参数\n",
    "在使用小批量梯度优化我们的模型参数之前，我们需要首先初始化模型参数，\n",
    "有了初始化的模型参数，我们就可以更新这些参数，直到这些参数足够拟合我们的数据。\n",
    "因为每次更新参数的时候都需要计算损失函数关于模型参数的梯度，\n",
    "以便向减小损失函数的方向更新模型参数，所以我们引入自动微分requires_grad来计算梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0051],\n",
       "         [0.0039]], requires_grad=True),\n",
       " tensor([0.], requires_grad=True))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 初始化模型参数\n",
    "w = torch.normal(0,0.01,(2,1),requires_grad=True) # 均值为0，方差为0.01的正态分布\n",
    "b = torch.zeros(1,requires_grad=True)\n",
    "w,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4 定义模型\n",
    "我们需要定义一个模型，将模型的输入和参数同模型的输出关联起来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg(X,w,b):\n",
    "    # 线性回归模型\n",
    "    return torch.matmul(X,w)+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5 定义损失函数\n",
    "均方损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(y_hat,y):\n",
    "    return (y_hat-y.reshape(y_hat.shape))**2 / 2 #没有计算均值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.6 定义优化算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, lr, batch_size):# params：是list，包含参数b和w，lr：学习率，\n",
    "    #小批量梯度下降\n",
    "    with torch.no_grad(): #更新的时候不采用梯度计算\n",
    "        for param in params:   # param： w或者b\n",
    "            param -= lr*param.grad/batch_size # 除以batch_size，相当于计算均值\n",
    "            param.grad.zero_() # 梯度清0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.7 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1,loss0.047493\n",
      "epoch2,loss0.000240\n",
      "epoch3,loss0.000055\n"
     ]
    }
   ],
   "source": [
    "lr = 0.03  # 学习率\n",
    "num_epochs = 3  # 整个数据扫三遍\n",
    "net  = linreg    # 模型\n",
    "loss = squared_loss  #损失函数\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter(batch_size,features,labels):\n",
    "        l = loss(net(X,w,b),y) # x和y的小批量损失\n",
    "        # 因为l的形状是（batch_size，1），而不是标量\n",
    "        #  计算关于【w,b】的梯度\n",
    "        l.sum().backward() # 求和之后算梯度\n",
    "        sgd([w,b],lr,batch_size) # 使用参数的梯度更新w,b\n",
    "        \n",
    "    with torch.no_grad():  # 打印时候，不用梯度\n",
    "        train_l = squared_loss(net(features,w,b),labels)\n",
    "        # print(train_l)\n",
    "        print(f'epoch{epoch+1},loss{float(train_l.mean()):f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.8 比较真实参数和通过训练学到的参数来评估训练的成功程度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w的估计误差：tensor([ 0.0007, -0.0007], grad_fn=<SubBackward0>)\n",
      "b的估计误差：tensor([0.0005], grad_fn=<RsubBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(f'w的估计误差：{true_w - w.reshape(true_w.shape)}')\n",
    "print(f'b的估计误差：{true_b - b}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归完整实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1,loss0.033085\n",
      "epoch2,loss0.000120\n",
      "epoch3,loss0.000054\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\noutput：\\nepoch1,loss0.362755\\nepoch2,loss0.008698\\nepoch3,loss0.000259\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "def synthetic_data(w, b, num_examples):\n",
    "    \"\"\"生成 y=Xw + b + 噪声\"\"\"\n",
    "    X = torch.normal(0, 1, (num_examples, len(w))) # 正态分布（均值为0，标准差为1）\n",
    "    y = torch.matmul(X, w) + b # 矩阵相乘\n",
    "    y += torch.normal(0, 0.01, y.shape) # 加入噪声项\n",
    "    # 得到的y为行向量的形式，为了使其变为一列的形式需要进行reshape\n",
    "    return X, y.reshape((-1, 1))\n",
    "\n",
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    # 这些样本是随机读出的，没有特定的顺序\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        batch_indices = torch.tensor(indices[i:min(i+batch_size,num_examples)])\n",
    "        yield features[batch_indices],labels[batch_indices]\n",
    "\n",
    "def linear(X,w,b):\n",
    "    \"\"\"定义模型\"\"\"\n",
    "    return torch.matmul(X,w)+b\n",
    "\n",
    "def squared_loss(y_hat,y):\n",
    "    return (y_hat-y.reshape(y_hat.shape))**2/2\n",
    "\n",
    "def sgd(params, lr, batch_size):\n",
    "    \"\"\"小批量梯度下降\"\"\"\n",
    "    with torch.no_grad():\n",
    "        for param in params:  # 参数b和w\n",
    "            param -= lr*param.grad/batch_size\n",
    "            param.grad.zero_()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    true_w = torch.tensor([2, -3.4])\n",
    "    true_b = 4.2\n",
    "    features, labels = synthetic_data(true_w,true_b,1000) # 生成数据集\n",
    "    # 初始化模型参数\n",
    "    w = torch.normal(0,0.01,(2,1),requires_grad=True)\n",
    "    b = torch.zeros(1,requires_grad=True)\n",
    "\n",
    "    # 定义超参数\n",
    "    lr = 0.03\n",
    "    num_epochs = 3\n",
    "    batch_size = 10\n",
    "    net = linear\n",
    "    loss = squared_loss\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for X, y in data_iter(batch_size,features,labels):\n",
    "            y_hat = linear(X,w,b)\n",
    "            loss = squared_loss(y_hat,y)\n",
    "            loss.sum().backward() # 进行反向传播得到梯度\n",
    "            sgd((w,b),lr,batch_size)\n",
    "        with torch.no_grad():\n",
    "            train_l = squared_loss(net(features,w,b),labels)\n",
    "            # print(train_l)\n",
    "            print(f'epoch{epoch+1},loss{float(train_l.mean()):f}')\n",
    "\n",
    "            \n",
    "\"\"\"\n",
    "output：\n",
    "epoch1,loss0.362755\n",
    "epoch2,loss0.008698\n",
    "epoch3,loss0.000259\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 2.线性回归简洁实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from d2l import torch as d2l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_data(w, b, num_examples):  \n",
    "    \"\"\"生成 y=Xw + b + 噪声\"\"\"\n",
    "    X = torch.normal(0, 1, (num_examples, len(w))) # 正态分布（均值为0，标准差为1），num_examples：n个样本，w：列数\n",
    "    y = torch.matmul(X, w) + b # 矩阵相乘\n",
    "    y += torch.normal(0, 0.01, y.shape) # 加入噪声项\n",
    "    # 得到的y为行向量的形式，为了使其变为一列的形式需要进行reshape\n",
    "    return X, y.reshape((-1, 1))  # x,y从行向量变为列向量\n",
    "\n",
    "true_w=torch.tensor([2,-3.4])\n",
    "true_b=4.2\n",
    "features, labels = synthetic_data(true_w,true_b,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.0276,  1.3525],\n",
       "         [-0.5965, -2.0344],\n",
       "         [ 1.3304, -0.7538],\n",
       "         [ 0.4916,  0.0371],\n",
       "         [ 1.0540,  1.2262],\n",
       "         [ 1.1513,  1.7523],\n",
       "         [-0.3448, -0.3764],\n",
       "         [-0.2247,  0.0912],\n",
       "         [ 0.7380, -0.6897],\n",
       "         [ 0.8165,  0.5144]]),\n",
       " tensor([[-0.4609],\n",
       "         [ 9.9160],\n",
       "         [ 9.4162],\n",
       "         [ 5.0823],\n",
       "         [ 2.1411],\n",
       "         [ 0.5540],\n",
       "         [ 4.8031],\n",
       "         [ 3.4300],\n",
       "         [ 8.0203],\n",
       "         [ 4.0828]])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_array(data_arrays, batch_size, is_train=True):\n",
    "    \"\"\"构建一个pytorch数据迭代器\"\"\"\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "    # shuffle= is_train=true：随机打乱；\n",
    "batch_size = 10\n",
    "data_iter = load_array((features,labels),batch_size)\n",
    "next(iter(data_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from torch.utils import data\n",
    "dataset   = data.TensorDataset(features, labels)\n",
    "data_iter = data.DataLoader(dataset, batch_size, shuffle=True)\n",
    "\n",
    "for i, data in enumerate (data_iter):\n",
    "    x_data, label = data\n",
    "    print(i,x_data,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "net = nn.Sequential(nn.Linear(2,1))  # pytorch 的word文档\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#更换权重\n",
    "net[0].weight.data.normal_(0,0.01) \n",
    "# data是真实的data；data.normal_(0,0.01)：将data换成(0,0.01)的正态分布。\n",
    "\n",
    "# 更换偏置\n",
    "net[0].bias.data.fill_(0)\n",
    "# data是真实的data；fill_(0)：将data换成0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 均方误差，也称L2范数\n",
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#实例化SGD实例\n",
    "trainer = torch.optim.SGD(net.parameters(),0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1,loss0.000235\n",
      "epoch2,loss0.000096\n",
      "epoch3,loss0.000094\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 3\n",
    "for epoch in range(num_epoch):\n",
    "    for X,y in data_iter:\n",
    "        l = loss(net(X),y)\n",
    "        trainer.zero_grad()\n",
    "        l.backward()\n",
    "        trainer.step() # 优化\n",
    "    l = loss(net(features),labels)\n",
    "    print(f'epoch{epoch+1},loss{l:f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一次性简洁实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1,loss0.000247\n",
      "epoch2,loss0.000106\n",
      "epoch3,loss0.000106\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "def synthetic_data(w, b, num_examples):  \n",
    "    \"\"\"生成 y=Xw + b + 噪声\"\"\"\n",
    "    X = torch.normal(0, 1, (num_examples, len(w))) # 正态分布（均值为0，标准差为1），num_examples：n个样本，w：列数\n",
    "    y = torch.matmul(X, w) + b # 矩阵相乘\n",
    "    y += torch.normal(0, 0.01, y.shape) # 加入噪声项\n",
    "    # 得到的y为行向量的形式，为了使其变为一列的形式需要进行reshape\n",
    "    return X, y.reshape((-1, 1))  # x,y从行向量变为列向量\n",
    "\n",
    "true_w=torch.tensor([2,-3.4])\n",
    "true_b=4.2\n",
    "features, labels = synthetic_data(true_w,true_b,1000)\n",
    "\n",
    "def load_array(data_arrays, batch_size, is_train=True):\n",
    "    \"\"\"构建一个pytorch数据迭代器\"\"\"\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "    # shuffle= is_train=true：随机打乱；\n",
    "batch_size = 10\n",
    "data_iter = load_array((features,labels),batch_size)\n",
    "# next(iter(data_iter))\n",
    "\n",
    "from torch import nn\n",
    "net = nn.Sequential(nn.Linear(2,1))   # pytorch 的word文档\n",
    "\n",
    "# net[0].weight.data,net[0].bias.data\n",
    "\n",
    "#更换权重\n",
    "net[0].weight.data.normal_(0,0.01) \n",
    "# data是真实的data；data.normal_(0,0.01)：将data换成(0,0.01)的正态分布。\n",
    "# 更换偏置\n",
    "net[0].bias.data.fill_(0)\n",
    "# data是真实的data；fill_(0)：将data换成0。\n",
    "\n",
    "# 均方误差，也称L2范数\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "#实例化SGD实例\n",
    "trainer = torch.optim.SGD(net.parameters(),0.03)\n",
    "\n",
    "num_epoch = 3\n",
    "for epoch in range(num_epoch):\n",
    "    for X,y in data_iter:\n",
    "        l = loss(net(X),y)\n",
    "        trainer.zero_grad()  # 梯度清零\n",
    "        l.backward()    # 得到梯度\n",
    "        trainer.step() # 模型更新\n",
    "    l = loss(net(features),labels)\n",
    "    print(f'epoch{epoch+1},loss{l:f}')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
